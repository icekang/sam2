{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "437ed43d-446c-4fd9-a1bf-7057284b976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6797e292-5ab0-4e1c-aaf0-7d0243628d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_points(coords, labels, ax, marker_size=200):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    return ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad1265-cbd3-49f7-8983-cd94e8686ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_every_n = 4\n",
    "prompting_method = 'consistent_10+1_point_prompts'\n",
    "for var in range(1, 5):\n",
    "    output_name = \"sam2.1_hiera_s_MOSE_finetune_scale4_var{}.yaml\".format(var)\n",
    "    for patient_id in ['101-045', '706-005']:\n",
    "        prediction_output = Path(f'./{output_name}_annotate_every_{annotation_every_n}_{prompting_method}/')\n",
    "        prediction_dir = prediction_output\n",
    "        prediction_path = prediction_dir / f'video_segments_{patient_id}.pt'\n",
    "        predictions = torch.load(prediction_path, weights_only=False)\n",
    "        \n",
    "        prompt_path = prediction_dir / f'prompts_{patient_id}.pt'\n",
    "        prompts = torch.load(prompt_path, weights_only=False)\n",
    "        frame_idx2prompt = {prompt['frame']: prompt for prompt in prompts}\n",
    "\n",
    "        groundtruth_dir = Path('/home/gridsan/nchutisilp/datasets/SAM2_Dataset302_Calcium_OCTv2/labelsTs')\n",
    "        groundtruth_path = groundtruth_dir / patient_id\n",
    "\n",
    "        image_dir = Path('/home/gridsan/nchutisilp/datasets/SAM2_Dataset302_Calcium_OCTv2/imagesTs')\n",
    "        image_path = image_dir / patient_id\n",
    "\n",
    "        color_pallete = np.array([[0, 0, 0], [0, 255, 0], [0, 255, 0]])\n",
    "\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(6, 2))\n",
    "\n",
    "        image_paths = list(image_path.glob('*.jpg'))\n",
    "        image_paths.sort()\n",
    "\n",
    "        groundtruth_paths = list(groundtruth_path.glob('*.png'))\n",
    "        groundtruth_paths.sort()\n",
    "\n",
    "        frame = 0\n",
    "\n",
    "\n",
    "        ax[1].set_title('Groundtruth')\n",
    "\n",
    "        def update(frame):\n",
    "            ax[2].clear()\n",
    "            ax[0].set_title(f'Image ({frame} | {image_paths[frame].name})')\n",
    "            \n",
    "            if frame % annotation_every_n == 0:\n",
    "                ax[2].set_title('SAM2 Prediction (mask prompted)')\n",
    "            else:\n",
    "                ax[2].set_title('SAM2 Prediction')\n",
    "\n",
    "            image = Image.open(image_paths[frame])\n",
    "            image = image.convert('RGBA')\n",
    "\n",
    "            gt = Image.open(groundtruth_paths[frame])\n",
    "            gt = gt.convert('RGBA')\n",
    "\n",
    "            prediction = Image.fromarray(color_pallete[predictions[frame][1][0].astype(np.uint8)].astype(np.uint8))\n",
    "            prediction = prediction.convert('RGBA')\n",
    "\n",
    "            new_image = Image.blend(gt, prediction, 0.5)\n",
    "            new_image = Image.blend(image, new_image, 0.5)\n",
    "            \n",
    "            ax_2_output = ax[2].imshow(prediction)\n",
    "            if frame in frame_idx2prompt:\n",
    "                prompt = frame_idx2prompt[frame]\n",
    "                prompt_type = prompt['type']\n",
    "                if prompt_type == 'mask':\n",
    "                    pass\n",
    "                elif prompt_type == 'point':\n",
    "                    points_xy = prompt['points']\n",
    "                    N, two = points_xy.shape\n",
    "                    ax_2_output = show_points(\n",
    "                        coords=points_xy,\n",
    "                        labels=np.array([1] * N),\n",
    "                        ax=ax[2], marker_size=200)\n",
    "            return ax[0].imshow(new_image), ax[1].imshow(gt), ax_2_output\n",
    "\n",
    "        update(0)\n",
    "        print(f'\\n{patient_id}_annotation_every_n_{annotation_every_n}_{output_name}_{prompting_method}')\n",
    "        ani = animation.FuncAnimation(fig=fig, func=update, frames=len(image_paths))        \n",
    "        ani.save(f'{patient_id}_annotation_every_n_{annotation_every_n}_{output_name}_{prompting_method}.mp4', progress_callback=lambda i, n: print('\\r', i, '/', n, end=''))\n",
    "        break\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-sam2]",
   "language": "python",
   "name": "conda-env-.conda-sam2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
